# Proyecto ELT con Airflow, DBT y PostgreSQL en AWS

Este proyecto implementa un pipeline de ingesta, modelado y an√°lisis de datos utilizando **Airflow**, **DBT**, **PostgreSQL** y **Docker**. Est√° dise√±ado como un entorno de pr√°ctica para la construcci√≥n de data pipelines modernos (arquitectura medall√≥n) y la exploraci√≥n de datasets como *Airbnb NYC* y datos del **BCRA**.

---

## üöÄ Tecnolog√≠as utilizadas

- **Python 3.8+**
- **PostgreSQL**
- **DBT (Data Build Tool)**
- **Apache Airflow**
- **Docker & Docker Compose**
- **PgAdmin**


---

## üìÇ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ assets/                        # Prints de pantalla y evidencia
‚îú‚îÄ‚îÄ airflow/                        # Configuraci√≥n de Airflow (DAGs, metadatos, logs)
‚îú‚îÄ‚îÄ dags/                           # DAGs de Airflow para orquestaci√≥n de procesos
‚îú‚îÄ‚îÄ data/                           # Datasets de entrada
‚îÇ   ‚îî‚îÄ‚îÄ AB_NYC.csv                  # Dataset de Airbnb NYC
‚îú‚îÄ‚îÄ dbt/                            # Proyecto de modelado anal√≠tico con DBT
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Modelos de dimensiones y hechos base
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_date.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_host.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_location.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_room_type.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fact_listing_snapshot.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ marts/                  # Modelos de negocio / KPIs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mart_avg_price_by_area.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mart_host_inventory_and_prices.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mart_reviews_trend_by_district.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ staging/                # Modelos de staging (limpieza y estandarizaci√≥n)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ stg_listings.sql
‚îÇ   ‚îú‚îÄ‚îÄ sources.yml                 # Definici√≥n de fuentes de datos
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml             # Configuraci√≥n principal de DBT
‚îÇ   ‚îú‚îÄ‚îÄ packages.yml                # Dependencias DBT
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml                # Perfil de conexi√≥n a PostgreSQL
‚îú‚îÄ‚îÄ logs/                           # Logs de ejecuci√≥n
‚îú‚îÄ‚îÄ scripts/                        # Scripts de ingesta y soporte
‚îÇ   ‚îú‚îÄ‚îÄ fetch_bcra.py               # Obtiene datos desde la API del BCRA
‚îÇ   ‚îú‚îÄ‚îÄ load_raw_to_rds.py          # Carga de datos crudos a PostgreSQL/RDS
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.airflow.yml  # Configuraci√≥n de contenedores con Airflow
‚îú‚îÄ‚îÄ nyc_elt_colab.ipynb             # Notebook de an√°lisis
‚îú‚îÄ‚îÄ Dockerfile                      # Imagen base del proyecto
‚îú‚îÄ‚îÄ entrypoint.sh                   # Script de inicializaci√≥n de contenedor
‚îú‚îÄ‚îÄ requirements.txt                # Dependencias Python
‚îî‚îÄ‚îÄ readme.md                       # Documentaci√≥n
‚îî‚îÄ‚îÄ Proyecto_Integrador_III.pdf                      # Documentaci√≥n principal del proyecto
```

---

## Archivos 

### Archivo Google Colab nyc_elt_colab.ipynb

En este archivo se puede ver el contenido de las principales tablas tanto de hecho como de dimensiones.
Tambien est√°n las consultas, an√°lisis y los gr√°ficos de las preguntas de negocio.


### Archivo Google Proyecto_Integrador_III.pdf

Aqui est√° documentado las siguientes elementos:
* Objetivo del proyecto
* Arquitectura
* Arquitectura t√©cnica
* Justificaci√≥n de la arquitectura
* Modelado de las tablas
* Evidencia Airflow y DBT



## üìä Flujo del Pipeline

1. **Extracci√≥n**
   - `fetch_bcra.py`: Llama a la API del BCRA y descarga series temporales econ√≥micas.
   - Dataset `AB_NYC.csv`: Datos de alojamientos de Airbnb en NYC.

2. **Carga**
   - `load_raw_to_rds.py`: Inserta datos crudos en PostgreSQL (local o RDS en AWS).
   - `docker-compose.airflow.yml`: Levanta contenedores de Airflow para orquestar las tareas de ETL.

3. **Transformaci√≥n**
   - DBT organiza el modelado en 3 capas:
     - **Staging**: limpieza y estandarizaci√≥n (`stg_`).
     - **Core**: dimensiones y hechos (`dim_`, `fact_`).
     - **Marts**: KPIs y m√©tricas de negocio (`mart_`).

4. **Orquestaci√≥n**
   - Airflow ejecuta los DAGs definidos en `/dags`, que automatizan extracci√≥n, carga y ejecuci√≥n de modelos DBT.

---

## Diagrama de Arquitectura

![Airflow Dags](assets/Diagrama_arquitectura.png)

## Diagrama T√©cnico de Arquitectura

![Airflow Dags](assets/Diagrama_tecnico.png)

## Instalaci√≥n y despliegue

## Despliegue en Local

### 1. Clonar repositorio
```bash
git clone <repo_url>
cd <repo_name>
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 3. Configurar variables de entorno
Crear archivo `.env`:
```env
POSTGRES_DB=nyc_project
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin123
PGADMIN_DEFAULT_EMAIL=admin@admin.com
PGADMIN_DEFAULT_PASSWORD=admin123
```

### 4. Levantar contenedores con Docker
```bash
docker-compose up -d
```

Servicios disponibles:
- **PostgreSQL:** `localhost:5432`
- **PgAdmin:** `http://localhost:8080`
- **Airflow UI:** `http://localhost:8081`

### 5. Ejecutar DBT
```bash
cd dbt
dbt run
dbt test
dbt docs generate
dbt docs serve --port 8082
```

### 6. Configuracion Variables necesarias

Completar con tus datos

```bash
export BCRA_BASE_URL="https://api.bcra.gob.ar/estadisticas/v2"
export BCRA_ENDPOINT="/principales_variables"
export BCRA_SERIES="usd_oficial,ipc" 
export BCRA_AUTH_TYPE="none"    
export BCRA_TOKEN=""                    
export BCRA_OUTPUT_PREFIX="BCRA"
export BCRA_LOAD_TO_RDS=true
export S3_BUCKET_NAME=""
export S3_PREFIX=AB_NYC
export PGHOST=""
export PGDATABASE=""
export PGUSER="postgres"
export PGPASSWORD=""
export PGPORT=5432
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY= "tu"
export AWS_REGION=""
```

### 6. Build Docker

```bash
docker build --no-cache -t nyc-elt:latest . 
```

### 7. Ejecutar Docker con Airflow

```bash
docker run --rm \
  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  -e AWS_REGION=us-east-1 \
  -e S3_BUCKET_NAME=$S3_BUCKET_NAME \
  -e PGHOST=$PGHOST -e PGDATABASE=$PGDATABASE \
  -e PGUSER=$PGUSER -e PGPASSWORD=$PGPASSWORD \
  -e PGPORT=5432 \
  -e BCRA_BASE_URL=https://api.bcra.gob.ar \
  -e BCRA_API_PATH=/estadisticas/v3.0 \
  -e BCRA_CAT_PATH=/Monetarias \
  -e BCRA_FROM=2025-08-29 -e BCRA_TO=2025-08-29 \
  -e BCRA_SERIES=usd_oficial \
  -e VERIFY_SSL=false \
  nyc-elt:latest
```

### 8. Ejecutar Airflow

Levantar los contenedores de Airflow Webserver y Scheduler

```bash
docker-compose -f docker-compose.airflow.yml up -d airflow-webserver airflow-scheduler
```

### 9. Ejecutar Airflow

Ingresar al sitio de Airflow

```bash
http://localhost:8081/
```


Nota: Para ver si hay algun error en las corridas de los DAGs correr este comando:

```bash
docker compose -f docker-compose.airflow.yml exec -it airflow-scheduler bash
```

## Despliegue en Entorno Virtual (AWS EC2)

### 1. Conectarse al servicio AWS EC2 por SSH

# Desde la terminal
```bash
ssh -i "my-ec2-key.pem" ec2-user@ec2-34-235-167-49.compute-1.amazonaws.com  
```

### 2. Clonar repositorio
```bash
git clone <repo_url>
cd <repo_name>
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Configuracion archivo .env

Crear archivo .env con tus datos

```bash
BCRA_BASE_URL="https://api.bcra.gob.ar/estadisticas/v2"
BCRA_ENDPOINT="/principales_variables"
BCRA_SERIES="usd_oficial,ipc" 
BCRA_OUTPUT_PREFIX="BCRA"
BCRA_LOAD_TO_RDS=true
S3_BUCKET_NAME=""
S3_PREFIX=AB_NYC
PGHOST=""
PGDATABASE=""
PGUSER="postgres"
PGPASSWORD=""
PGPORT=5432
AWS_ACCESS_KEY_ID="tus_datos"
AWS_SECRET_ACCESS_KEY= "tus_datos"
AWS_REGION=""
```


### 5. Actualizar librerias necesarias en EC2

```bash
# Actualizar
sudo dnf update -y
```

```bash
# Verificar
docker-compose --version
```

### 6. Build Docker

### Instalar Docker y levantarlo
```bash
sudo yum update -y
sudo amazon-linux-extras enable docker
sudo yum install docker -y
sudo systemctl start docker
sudo usermod -aG docker ec2-user

# docker-compose plugin
sudo curl -L https://github.com/docker/compose/releases/download/v2.24.7/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

```bash
docker build --no-cache -t nyc-elt:latest . 
```

### 7. Ejecutar Docker con Airflow

```bash
docker run --rm \
  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  -e AWS_REGION=us-east-1 \
  -e S3_BUCKET_NAME=$S3_BUCKET_NAME \
  -e PGHOST=$PGHOST -e PGDATABASE=$PGDATABASE \
  -e PGUSER=$PGUSER -e PGPASSWORD=$PGPASSWORD \
  -e PGPORT=5432 \
  -e BCRA_BASE_URL=https://api.bcra.gob.ar \
  -e BCRA_API_PATH=/estadisticas/v3.0 \
  -e BCRA_CAT_PATH=/Monetarias \
  -e BCRA_FROM=2025-08-29 -e BCRA_TO=2025-08-29 \
  -e BCRA_SERIES=usd_oficial \
  -e VERIFY_SSL=false \
  nyc-elt:latest
```


### 8. Instalar librerias Airflow Compose

```bash
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

### 9. Ejecutar Airflow

Levantar los contenedores de Airflow Webserver y Scheduler

```bash
docker-compose -f docker-compose.airflow.yml up -d airflow-webserver airflow-scheduler
```


Abr√≠ el puerto 8081 en el Security Group de la EC2 y entr√° a
http://<IP_PUBLICA_EC2>:8081  con el usuario admin y el pass admin.

### Evidencia Airflow


![Airflow Dags](assets/Airflow_dags.png)
![Airflow Rung](assets/Airflow_run.png)
---

## üìà Modelos

- **Core (dim/fact):**
  - `dim_date`, `dim_host`, `dim_location`, `dim_room_type`, `fact_listing_snapshot`

- **Marts (business-ready):**
  - `mart_avg_price_by_area`
  - `mart_host_inventory_and_prices`
  - `mart_reviews_trend_by_district`
  - `mart_roomtype_supply_and_rev`
  - `mart_price_distribution_outliers`

Ejemplos de m√©tricas:
- Precio promedio por √°rea
- Concentraci√≥n de listings por vecindario
- Tendencia de rese√±as por distrito
- Outliers de precios
- Relaci√≥n entre disponibilidad y reviews

---

### Documentacion DBT

La capa Staging act√∫a como el primer nivel de transformaci√≥n sobre los datos crudos. Toma las tablas provenientes de la capa Bronze (en este caso la fuente raw.ab_nyc) y aplica procesos de limpieza, estandarizaci√≥n y control de calidad para dejarlos listos para integraciones posteriores.

La capa Core constituye el modelo dimensional del proyecto. A partir de los datos limpios y estandarizados de Staging, aqu√≠ se construyen dimensiones y tablas de hechos que permiten consultas eficientes y consistentes para an√°lisis de negocio.

La capa Marts constituye la capa de negocio (Gold) del modelo. A partir de las dimensiones y hechos de la capa Core, construye vistas anal√≠ticas que exponen indicadores clave (KPIs) y m√©tricas directamente consumibles por analistas, dashboards o reportes.

### Evidencia DBT

![DBT Staging](assets/dbt_raw.png)
![DBT Core y Marts](assets/dbt_core.png)



## GitHub Actions

### Archivo .github/workflows/ci.yml

Tareas que realiza:
* Se dispara en todo push o PR.
* Levanta la base de datos Postgres.
* Instala Python y dbt.
* Descarga dependencias de dbt.
* Carga data/AB_NYC.csv en raw.ab_nyc.
* Ejecuta dbt debug y dbt build con tests.

Para ejecutar:

```bash
git push origin feature
```

### Archivo .github/workflows/deploy-ec2.yml

Tareas que realiza:

* Se conecta por SSH a tu EC2.
* Entra al directorio del repo, hace git reset --hard origin/main.
* Build de im√°genes definidas en tu docker-compose.airflow.yml.
* Init del metastore de Airflow
* Levanta webserver y scheduler.

Para ejecutar:

```bash
git checkout -b feature/mis_cambios
git add -A
git commit -m "Cambios realizados"
git push -u origin feature/mis_cambios
```



## Troubleshooting

1- Error falta espacio: Si te quedas sin espacio en el servidor virtual correr:
```bash
docker system prune -af para limpiar.
```


2- Error caracteres EC2:
```bash
sudo localedef -i en_US -f UTF-8 en_US.UTF-8
```

3- Ver logs del Airflow:
```bash
docker compose -f docker-compose.airflow.yml exec -it airflow-scheduler bash
```

4- Ver errores del Airflow por fecha:
```bash
docker compose -f docker-compose.airflow.yml exec airflow-scheduler \
  bash -lc 'airflow tasks test nyc_elt_bcra_dbt fetch_bcra 2025-08-30T23:29:18+00:00'
```

5- Error Base de datos

Verificar que est√© cargada tu IP p√∫blica antes de correr el proyecto.

6- Aumentar el tama√±o del volumen EBS desde AWS Console:
```bash
#Obtener el Volume ID de tu EC2

aws ec2 describe-instances --instance-ids i-0cb657d54b9daef96 --query 'Reservations[0].Instances[0].BlockDeviceMappings[0].Ebs.VolumeId' --output text

#Aumentar el volumen:
VOLUME_ID="vol-xxxxxxxxx"

# Aumentar a 20GB (ajusta seg√∫n necesites)
aws ec2 modify-volume --volume-id $VOLUME_ID --size 20
```

## üë®‚Äçüíª Autor

Desarrollado por **Diego Lopez Castan** como ejercicio integrador de orquestaci√≥n (Airflow), modelado anal√≠tico (DBT) y despliegue en contenedores (Docker).

---

## üìú Licencia

Uso libre para fines educativos y personales.
